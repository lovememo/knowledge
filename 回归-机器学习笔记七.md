<span style="color: #f60;"><em><strong>转载请邮件告知lovememo@foxmail.com并在博文首行醒目注明出处并链接至本博文</strong></em></span>

回归
机器学习
<h2>一、用线性回归找到最佳拟合直线</h2>
性能度量采用均方误差：
$$E(\bold{\it{\theta}}) = \sum_{i=1}^m(y_i-\bold{\it{x}_i^T\bold{\it{\theta})^2\tag{8.1}$$
化为矩阵形式即为：
$$E(\bold{\it{\theta}) = (\bold{\it{y} - \bold{\it{X}\bold{\it{\theta})^T(\bold{\it{y} - \bold{\it{X}\bold{\it{\theta})\tag{8.2}$$
对$\bold{\it{\theta}$求导得：
$$\frac{\partial E(\bold{\it{\theta})}{\partial \bold{\it{\theta}} = 2\bold{\it{X}^T(\bold{\it{X}\bold{\it{\theta} - \bold{\it{y})\tag{8.3}$$
令$\frac{\partial E(\bold{\it{\theta})}{\partial \bold{\it{\theta}} = 0$,求得：
$$\hat{\bold{\it{\theta}} =  (\bold{\it{X}^T\bold{\it{X})^{-1}\bold{\it{X}^T\bold{\it{y} \tag{8.4}$$


关于均方误差的矩阵形式求导(参考自<a href="https://segmentfault.com/a/1190000007456184#articleHeader3" target="_blank">coder_ss</a>)：
<br>$$ \begin{align*} \frac{\partial E(\bold{\it{\theta})}{\partial \bold{\it{\theta}} & = \frac{\partial \left( (\bold{\it{y} - \bold{\it{X}\bold{\it{\theta})^T(\bold{\it{y} - \bold{\it{X}\bold{\it{\theta}) \right)}{\partial \bold{\it{\theta}} \\ & = \frac{\partial \left( (\bold{\it{y}^T - \bold{\it{\theta}^T\bold{\it{X}^T)(\bold{\it{y} - \bold{\it{X}\bold{\it{\theta}) \right)}{\partial \bold{\it{\theta}} \\ & = \frac{\partial \left( \bold{\it{y}^T\bold{\it{y} - \bold{\it{\theta}^T\bold{\it{X}^T \bold{\it{y} - \bold{\it{y}^T\bold{\it{X}\bold{\it{\theta} + \bold{\it{\theta}^T\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta} \right)}{\partial \bold{\it{\theta}} \\ & = \frac{\partial (\bold{\it{y}^T\bold{\it{y}) }{\partial \bold{\it{\theta}} - \frac{\partial (\bold{\it{\theta}^T\bold{\it{X}^T \bold{\it{y}) }{\partial \bold{\it{\theta}} - \frac{\partial (\bold{\it{y}^T\bold{\it{X}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} + \frac{\partial (\bold{\it{\theta}^T\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} \\ & =0 - \bold{\it{X}^T \bold{\it{y} - \bold{\it{X}^T \bold{\it{y} + 2\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta} \\ & = 2\bold{\it{X}^T(\bold{\it{X}\bold{\it{\theta} - \bold{\it{y}) \end{align*} $$<br>其中，$\bold{\it{\theta}^T\bold{\it{X}^T \bold{\it{y}$、$\bold{\it{y}^T\bold{\it{X}\bold{\it{\theta}$、$\bold{\it{\theta}^T\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta}$ 都是标量（1×1的矩阵），所以可以分解成三个【标量对向量求导】的问题。我们知道对于含n特征的m个样本，$\bold{\it{\theta}$、$\bold{\it{X}$、$\bold{\it{y}$  的维度分别是(n+1)×1、 m×(n+1)、m×1，为方便讨论，我们这里认为它们的维度分别为n×1、 m×n、m×1。三个标量分别对向量求导如下：<br>$$ \begin{align*} \frac{\partial (\bold{\it{\theta}^T\bold{\it{X}^T \bold{\it{y}) }{\partial \bold{\it{\theta}} \overset{\bold{\it{X}^T \bold{\it{y} = \bold{\it{P}}{\Longrightarrow} \frac{\partial (\bold{\it{\theta}^T\bold{\it{P}) }{\partial \bold{\it{\theta}} & = \frac{\partial}{\partial \bold{\it{\theta}}  \begin{pmatrix} w_1 & w_2 & \cdots & w_n \end{pmatrix}   \begin{pmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{pmatrix}  \\ & = \frac{\partial}{\partial \bold{\it{\theta}} \sum_{i=1}^{n}w_i p_i =  \begin{pmatrix} \frac{\partial}{\partial w_1} \sum_{i=1}^{n}w_i p_i \\ \frac{\partial}{\partial w_2} \sum_{i=1}^{n}w_i p_i \\ \vdots \\ \frac{\partial}{\partial w_n} \sum_{i=1}^{n}w_i p_i \end{pmatrix}  =  \begin{pmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{pmatrix}  = \bold{\it{P} = \bold{\it{X}^T \bold{\it{y} \\ \\ \frac{\partial (\bold{\it{y}^T\bold{\it{X}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} \overset{\bold{\it{y}^T\bold{\it{X} = \bold{\it{P}}{\Longrightarrow} \frac{\partial (\bold{\it{P}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} & = \frac{\partial}{\partial \bold{\it{\theta}}   \begin{pmatrix} p_1 & p_2 & \cdots & p_n \end{pmatrix}   \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{pmatrix}  \\ & = \frac{\partial}{\partial \bold{\it{\theta}} \sum_{i=1}^{n}p_i w_i =  \begin{pmatrix} \frac{\partial}{\partial w_1} \sum_{i=1}^{n}p_i w_i \\ \frac{\partial}{\partial w_2} \sum_{i=1}^{n}p_i w_i \\ \vdots \\ \frac{\partial}{\partial w_n} \sum_{i=1}^{n}p_i w_i \end{pmatrix}  =  \begin{pmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{pmatrix}  = \bold{\it{P}^T = (\bold{\it{y}^T\bold{\it{X})^T =\bold{\it{X}^T \bold{\it{y} \\ \\ \frac{\partial (\bold{\it{\theta}^T\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} \overset{\bold{\it{X}^T\bold{\it{X} = \bold{\it{P}}{\Longrightarrow} \frac{\partial (\bold{\it{\theta}^T\bold{\it{P}\bold{\it{\theta}) }{\partial \bold{\it{\theta}} & = \frac{\partial}{\partial \bold{\it{\theta}}  \begin{pmatrix} w_1 & w_2 & \cdots & w_n \end{pmatrix}   \begin{pmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \end{pmatrix}   \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{pmatrix}  \\ & = \frac{\partial}{\partial \bold{\it{\theta}} \sum_{i=1}^{n} \sum_{j=1}^{n}w_i p_{ij} w_j =  \begin{pmatrix} \frac{\partial}{\partial w_1} \sum_{i=1}^{n} \sum_{j=1}^{n}w_i p_{ij} w_j \\ \frac{\partial}{\partial w_2} \sum_{i=1}^{n} \sum_{j=1}^{n}w_i p_{ij} w_j \\ \vdots \\ \frac{\partial}{\partial w_n} \sum_{i=1}^{n} \sum_{j=1}^{n}w_i p_{ij} w_j \end{pmatrix}  =  \begin{pmatrix} 2\sum_{i=1}^{n}p_{1i}w_i \\ 2\sum_{i=1}^{n}p_{2i}w_i \\ \vdots \\ 2\sum_{i=1}^{n}p_{ni}w_i \end{pmatrix}  \\ & = 2  \begin{pmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \end{pmatrix}   \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{pmatrix}  = 2\bold{\it{P}\bold{\it{\theta} = 2\bold{\it{X}^T\bold{\it{X}\bold{\it{\theta} \end{align*}$$<br>其中，$\frac{\partial}{\partial w_k} \sum \limits_{i=1}^{n} \sum \limits_{j=1}^{n}w_i p_{ij} w_j$ 的求导需要详细叙述一下，注意到$p_{ik} = p_{ki}$，求导过程如下：<br>$$ \begin{align*} \frac{\partial}{\partial w_k} \sum_{i=1}^{n} \sum_{j=1}^{n}w_i p_{ij} w_j & = \frac{\partial}{\partial w_k} \left( \sum_{i=k}^{k} \sum_{j=k}^{k}w_i p_{ij} w_j + \sum_{i=k}^{k} \sum_{j=1,j\neq k}^{n}w_i p_{ij} w_j + \sum_{i=1,i\neq k}^{n} \sum_{j=k}^{k}w_i p_{ij} w_j + \sum_{i=1,i\neq k}^{n} \sum_{j=1,j\neq k}^{n}w_i p_{ij} w_j \right) \\ & = \frac{\partial}{\partial w_k} \left( p_{kk} w_k^2 + \sum_{j=1,j\neq k}^{n}w_k p_{kj} w_j + \sum_{i=1,i\neq k}^{n} w_i p_{ik} w_k + \sum_{i=1,i\neq k}^{n} \sum_{j=1,j\neq k}^{n}w_i p_{ij} w_j \right) \\ & = 2p_{kk}w_k +  \sum_{j=1,j\neq k}^{n} p_{kj} w_j + \sum_{i=1,i\neq k}^{n} w_i p_{ik} + 0 \\ & =  \sum_{i=1}^{n} p_{ki} w_i + \sum_{i=1}^{n} p_{ik}w_i \\ & = 2\sum_{i=1}^{n} p_{ki}w_i \end{align*}$$


计算回归系数$\hat{\bold{\it{\theta}}$，即$(\bold{\it{X}^T\bold{\it{X})^{-1}\bold{\it{X}^T\bold{\it{y}$的值，代码如下：
```markdown
# -*- coding: utf-8 -*-
from numpy import *

def standRegres(xArr,yArr):
    xMat = mat(xArr)
    yMat = mat(yArr).T #行向量转列向量
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0: #计算行列式
        print "This matrix is singular, cannot do inverse" #奇异矩阵不可逆
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws

def loadDataSet(fileName):      #general function to parse tab -delimited floats
    numFeat = len(open(fileName).readline().split('\t')) - 1 #get number of fields 
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t') #strip -> trim
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1])) # -1 -> last element
    return dataMat,labelMat
```
绘图验证回归系数，计算$\hat{\bold{\it{y}}$与$\bold{\it{y}$的<a href="https://www.zhihu.com/question/20852004" target="_blank">相关系数</a>。
代码如下：
```markdown
# -*- coding: utf-8 -*-
import regression
import matplotlib.pyplot as plt

from numpy import *
xArr, yArr = regression.loadDataSet('ex0.txt')
ws = regression.standRegres(xArr, yArr)


xMat=mat(xArr)
yMat=mat(yArr)
fig = plt.figure()
ax = fig.add_subplot(111) #将画布分成1行1列，将从左到右，从上到下第一块显示图画
ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0],c='purple',label='realData',marker='.')
#scatter散点图
# matrix[a:b,c:d]  第a到b行，且第c到d列 左闭右开
# matrix[a,b] 第a行，第b列
xCopy=xMat.copy()
#xCopy.sort(0)#y轴方向排序
yHat=xCopy*ws
ax.plot(xCopy[:,1],yHat,c='green')

plt.show()

correlation = corrcoef(yHat.T, yMat )
print correlation
```

计算得到的相关系数矩阵为：$\begin{pmatrix}1. & 0.98647356\\ 0.98647356 & 1.\end{pmatrix}$图像为：
<img src="http://www.lovememo.net/wp-content/uploads/2017/08/figure_1.png" alt="fig8.1 最佳线性拟合直线" />

<h2>二、局部加权线性回归</h2>
    局部加权线性回归与标准线性回归的区别在于其预测每个$y_i$的值时，必须计算全量样本数据对应的权重对角矩阵，并根据全量的样本数据计算对应的回归系数$\theta_i$。我理解的其核心思想在于：通过<span style="color: #f60;">对“预测点与其他样本点的欧几里德距离”上增加权重</span>，来增强预测点附近的样本点对回归系数$\theta_i$影响，从而实现增强局部数据的拟合度。

性能度量为经过加权的均方误差：
$$E(\bold{\it{\theta}) = \sum_{i=1}^mw_i(y_i-\bold{\it{x}_i^T\bold{\it{\theta})^2 = \sum_{i=1}^m(\sqrt{w_i}y_i-\sqrt{w_i}\bold{\it{x}_i^T\bold{\it{\theta})^2\tag{8.5}$$
化为矩阵形式即为,其中$\bold{\it{W}$为对角矩阵：
$$E(\bold{\it{\theta}) = (\sqrt{\bold{\it{W}}\bold{\it{y} - \sqrt{\bold{\it{W}}\bold{\it{X}\bold{\it{\theta})^T(\sqrt{\bold{\it{W}}\bold{\it{y} - \sqrt{\bold{\it{W}}\bold{\it{X}\bold{\it{\theta})\tag{8.6}$$
不妨令$\bold{\it{y'} = \sqrt{\bold{\it{W}}\bold{\it{y}$, $\bold{\it{X'} = \sqrt{\bold{\it{W}}\bold{\it{X}$,则上式化为：
$$E(\bold{\it{\theta}) = (\bold{\it{y'} - \bold{\it{X'}\bold{\it{\theta})^T(\bold{\it{y'} - \bold{\it{X'}\bold{\it{\theta})\tag{8.7}$$
公式8.7与第一节的公式8.4形式一致，同理，获得$\hat{\bold{\it{\theta}}$：<br>$$ \begin{align*} \hat{\bold{\it{\theta}} & = (\bold{\it{X'}^T\bold{\it{X'})^{-1}\bold{\it{X'}^T\bold{\it{y'} \\ & = \left [(\sqrt{\bold{\it{W}}\bold{\it{X})^T\sqrt{\bold{\it{W}}\bold{\it{X}\right ]^{-1}(\sqrt{\bold{\it{W}}\bold{\it{X})^T\sqrt{\bold{\it{W}}\bold{\it{y}\\& =
 (\bold{\it{X}^T\sqrt{\bold{\it{W^T}}\sqrt{\bold{\it{W}}\bold{\it{X})^{-1}
\bold{\it{X}^T\sqrt{\bold{\it{W^T}}\sqrt{\bold{\it{W}}\bold{\it{y}\\& =(\bold{\it{X}^T\bold{\it{W}\bold{\it{X})^{-1}
\bold{\it{X}^T\bold{\it{W}\bold{\it{y}\tag{8.8}
 \end{align*}$$
上述的对角矩阵$\bold{\it{W}$是根据待预测的$\bold{\it{x}$与样本数据集$\bold{\it{X}$计算而来，具体到每个$\bold{\it{w_{ii}}$是由待预测的$\bold{\it{x}$与样本数据集中的每个$\bold{\it{x_i}$计算而来。这种计算逻辑的指导思想是使得靠近预测$\bold{\it{x}$的样本数据产生更高的欧几里得距离权重，这种计算逻辑公式被成为“核”，局部加权回归常用的核为“高斯核”（与高斯关系不大）：
$$w_{ii} = exp\left (\frac{\left |\bold{\it{x_i}-\bold{\it{x} \right |}{-2k^2}\right)$$
上述逻辑的实现代码如下所示，其中包含了根据点$(1,0.5)$在参数k不同值的情况下，生成的样本数据的权重曲线
```markdown
def getGaussWeightMat(fixedPoint, inputPoints, k=1):
    m = shape(inputPoints)[0]#对样本数据的每一个元素进行加权，所以权重对角矩阵的长度肯定是样本数据个数
    gaussWeightMat = mat(eye(m))#权重对角矩阵 参看numpy.eye
    for i in range(m):
        diffMat =  fixedPoint - inputPoints[i,:]
        gaussWeightMat[i,i] = exp(diffMat * diffMat.T / (-2 * k ** 2))
    return gaussWeightMat

def getGaussWeightArr(fixedPoint, inputPoints, k=1):
    gaussWeightMat = getGaussWeightMat(fixedPoint, inputPoints, k)
    m = shape(gaussWeightMat)[0]
    gaussWeightArr = []
    for i in range(m):
        gaussWeightArr.append(gaussWeightMat[i,i])
    return gaussWeightArr

#绘制权重曲线
def testGaussWeight():
    xArr, yArr = re.loadDataSet('ex0.txt')
    xMat = mat(xArr)
    yMat = mat(yArr)
    
    fig = plt.figure()
    ax = fig.add_subplot(411) #将画布分成1行1列，将从左到右，从上到下第一块显示图画
    plt.yticks(linspace(2.5, 5, 6))
    ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0],c='purple',label='realData',marker='.')
    
    xMat.sort(0)
    weights = re.getGaussWeightArr([1,0.5], xMat, 0.5)
    ax = fig.add_subplot(412)
    plt.yticks(linspace(0.6, 1, 9))
    ax.plot(xMat[:,1], weights)
    plt.text(0.7, 0.75, r'$k=0.5$')
    
    weights = re.getGaussWeightArr([1,0.5], xMat, 0.1)
    ax = fig.add_subplot(413)
    plt.yticks(linspace(0, 1, 6))
    ax.plot(xMat[:,1], weights)
    plt.text(0.7, 0.5, r'$k=0.1$')
    
    weights = re.getGaussWeightArr([1,0.5], xMat, 0.01)
    ax = fig.add_subplot(414)
    plt.yticks(linspace(0, 1, 6))
    ax.plot(xMat[:,1], weights, label='k=0.01')    
    plt.text(0.7, 0.5, r'$k=0.01$')
    plt.show()
```
生成曲线如下：
<img src="http://www.lovememo.net/wp-content/uploads/2017/08/figure_2.png" alt="权重曲线" />

用lwlr算法拟合：
```markdown
def testLwlr():
    xArr, yArr = re.loadDataSet('ex0.txt')
    xMat = mat(xArr)
    yMat = mat(yArr)    
    
    fig = plt.figure()
    ax = fig.add_subplot(311) #将画布分成1行1列，将从左到右，从上到下第一块显示图画
    privateShow(ax, xMat, yMat, 1.0)
    
    ax = fig.add_subplot(312)
    privateShow(ax, xMat, yMat, 0.02)
    
    ax = fig.add_subplot(313)
    privateShow(ax, xMat, yMat, 0.002)
    plt.show()
    
def privateShow(ax, xMat, yMat, k):
    ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0],c='purple',label='realData',marker='.')
    ind = xMat[:,1].argsort(0)#获取按第一列排序后的索引
    newXmat = xMat[ind][:,0,:]#按索引排序后会升一个维度，所以降一个维度
    newYmat = yMat.T[ind][:,0,:].T
    yHat = re.lwlrTest(newXmat, newXmat.A, newYmat.A, k)
    ax.plot(newXmat[:,1], yHat, c='green')    
```
拟合图像效果：
<img src="http://www.lovememo.net/wp-content/uploads/2017/08/figure_3.png" alt="局部加权线回归与性过拟合" />